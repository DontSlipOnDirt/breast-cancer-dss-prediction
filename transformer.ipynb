{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Data\n",
    "train_data_path = 'data/train_data.csv'\n",
    "pseudo_train_data_path = 'data/combined_labeled_data.csv'\n",
    "test_data_path = 'data/test_data.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "pseudo_train_data = pd.read_csv(pseudo_train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "train_data = train_data.drop(columns=['DssTime','Event'])\n",
    "test_data = test_data.drop(columns=['DssTime','Event'])\n",
    "\n",
    "# # Scale Data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset class\n",
    "class CancerDataset(Dataset):\n",
    "    def __init__(self, data, features, label_column):\n",
    "        self.data = data\n",
    "        self.features = features\n",
    "        self.label_column = label_column\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inputs = torch.tensor(self.data[self.features].iloc[idx].values, dtype=torch.float32)\n",
    "        label = torch.tensor(self.data[self.label_column].iloc[idx], dtype=torch.float32)\n",
    "        return inputs, label\n",
    "\n",
    "# Define feature columns\n",
    "# all_columns = ['ESR1', 'PGR', 'ERBB2', 'MKI67', 'PLAU', 'ELAVL1', 'EGFR', 'BTRC', 'FBXO6', 'SHMT2', 'KRAS', \n",
    "#                'SRPK2', 'YWHAQ', 'PDHA1', 'EWSR1', 'ZDHHC17', 'ENO1', 'DBN1', 'PLK1', 'GSK3B', 'Age', 'Size', \n",
    "#                'Menopausal State', 'Radio Therapy', 'Chemotherapy', 'Hormone Therapy', 'Neoplasm Histologic Grade',\n",
    "#                'Cellularity', 'Surgery-breast conserving', 'Surgery-mastectomy']\n",
    "train_columns = train_data.columns\n",
    "label_column = \"Label\"\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = CancerDataset(train_data, train_columns, label_column)\n",
    "test_dataset = CancerDataset(test_data, train_columns, label_column)  # Use same columns as during training\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unified Transformer binary classifier\n",
    "class UnifiedTransformerBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, transformer_dim=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(UnifiedTransformerBinaryClassifier, self).__init__()\n",
    "        \n",
    "        # Define the transformer model\n",
    "        self.embedding = nn.Linear(input_dim, transformer_dim)  # Linear layer to project input to transformer dim\n",
    "        \n",
    "        # Transformer Encoder Layer\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=num_heads, dropout=dropout),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(transformer_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply the initial embedding to get input in the transformer dimension\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add a batch dimension (needed for the transformer)\n",
    "        x = x.unsqueeze(1)  # Shape: [batch_size, 1, input_dim]\n",
    "        \n",
    "        # Pass through the transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Get the output from the transformer (we'll use the last output for classification)\n",
    "        x = x[:, -1, :]  # Shape: [batch_size, transformer_dim]\n",
    "        \n",
    "        # Classification layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Sigmoid activation for binary classification\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train_model(model, train_loader, epochs, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Send data to the device (GPU/CPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze(1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcmuf\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 0.8123042345046997\n",
      "Epoch 2/30, Loss: 0.7097771326700847\n",
      "Epoch 3/30, Loss: 0.6917547782262167\n",
      "Epoch 4/30, Loss: 0.6845372398694356\n",
      "Epoch 5/30, Loss: 0.6677679459253947\n",
      "Epoch 6/30, Loss: 0.6779388586680094\n",
      "Epoch 7/30, Loss: 0.6515095988909404\n",
      "Epoch 8/30, Loss: 0.6496336698532105\n",
      "Epoch 9/30, Loss: 0.610536766052246\n",
      "Epoch 10/30, Loss: 0.5673736055692037\n",
      "Epoch 11/30, Loss: 0.5049219210942586\n",
      "Epoch 12/30, Loss: 0.4391743838787079\n",
      "Epoch 13/30, Loss: 0.43664355874061583\n",
      "Epoch 14/30, Loss: 0.3910779962937037\n",
      "Epoch 15/30, Loss: 0.3021206135551135\n",
      "Epoch 16/30, Loss: 0.20717207367221516\n",
      "Epoch 17/30, Loss: 0.09863994481662909\n",
      "Epoch 18/30, Loss: 0.17177195213735102\n",
      "Epoch 19/30, Loss: 0.12560502427319686\n",
      "Epoch 20/30, Loss: 0.30115052685141563\n",
      "Epoch 21/30, Loss: 0.13317374462882678\n",
      "Epoch 22/30, Loss: 0.08182375188916921\n",
      "Epoch 23/30, Loss: 0.13373701988408962\n",
      "Epoch 24/30, Loss: 0.05785323406259219\n",
      "Epoch 25/30, Loss: 0.0222954036667943\n",
      "Epoch 26/30, Loss: 0.04253771541019281\n",
      "Epoch 27/30, Loss: 0.05348196079333623\n",
      "Epoch 28/30, Loss: 0.012486369318018357\n",
      "Epoch 29/30, Loss: 0.013420136459171773\n",
      "Epoch 30/30, Loss: 0.006346691120415926\n"
     ]
    }
   ],
   "source": [
    "# Define the model, optimizer, and loss function\n",
    "# torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyper parameters\n",
    "transformer_dim=64\n",
    "num_heads=4\n",
    "num_layers=2\n",
    "dropout=0.1\n",
    "\n",
    "input_dim = len(train_columns)  # Number of input features\n",
    "model = UnifiedTransformerBinaryClassifier(input_dim, transformer_dim, num_heads, num_layers, dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# Train the model\n",
    "epochs = 30\n",
    "train_model(model, train_loader, epochs, optimizer, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze(1)  # Shape: [batch_size]\n",
    "            preds = (outputs > 0.5).float()  # Binary classification (threshold 0.5)\n",
    "            \n",
    "            # Collect all predictions and true labels\n",
    "            all_preds.extend(preds.cpu().numpy())  # Move to CPU and convert to numpy\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert lists to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1-Score: {f1:.4f}')\n",
    "    if auc is not None:\n",
    "        print(f'AUC: {auc:.4f}')\n",
    "    else:\n",
    "        print('AUC: Not available (single class prediction)')\n",
    "    \n",
    "    return accuracy, precision, recall, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1-Score: 1.0000\n",
      "AUC: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0, 1.0, 1.0, 1.0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluate_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
