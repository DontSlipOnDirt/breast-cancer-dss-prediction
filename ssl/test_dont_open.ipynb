{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not open this\n",
    "\n",
    "![alt text](https://media.tenor.com/J7B7Y3Dz6GMAAAAC/dont-look-at-me.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the VIMELightning Module\n",
    "from ts3l.pl_modules import VIMELightning\n",
    "from ts3l.utils.vime_utils import VIMEDataset\n",
    "from ts3l.utils import TS3LDataModule, get_category_cardinality\n",
    "from ts3l.utils.vime_utils import VIMEConfig\n",
    "from ts3l.utils.embedding_utils import IdentityEmbeddingConfig\n",
    "from ts3l.utils.backbone_utils import MLPBackboneConfig\n",
    "from pytorch_lightning import Trainer\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (100, 24)\n",
      "Test data shape: (100, 24)\n",
      "Unlabelled data shape: (500, 23)\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.DataFrame({\n",
    "    'gene1': np.random.rand(100),\n",
    "    'gene2': np.random.rand(100),\n",
    "    'gene3': np.random.rand(100),\n",
    "    'gene4': np.random.rand(100),\n",
    "    'gene5': np.random.rand(100),\n",
    "    'gene6': np.random.rand(100),\n",
    "    'gene7': np.random.rand(100),\n",
    "    'gene8': np.random.rand(100),\n",
    "    'gene9': np.random.rand(100),\n",
    "    'gene10': np.random.rand(100),\n",
    "    'gene11': np.random.rand(100),\n",
    "    'gene12': np.random.rand(100),\n",
    "    'gene13': np.random.rand(100),\n",
    "    'gene14': np.random.rand(100),\n",
    "    'gene15': np.random.rand(100),\n",
    "    'gene16': np.random.rand(100),\n",
    "    'gene17': np.random.rand(100),\n",
    "    'gene18': np.random.rand(100),\n",
    "    'gene19': np.random.rand(100),\n",
    "    'gene20': np.random.rand(100),\n",
    "    'clinical1': np.random.randint(0, 2, size=100),\n",
    "    'clinical2': np.random.randint(0, 2, size=100),\n",
    "    'clinical3': np.random.randint(0, 5, size=100),\n",
    "    'Label': np.random.randint(0, 2, size=100),\n",
    "})\n",
    "train_data = pd.DataFrame({\n",
    "    'gene1': np.random.rand(100),\n",
    "    'gene2': np.random.rand(100),\n",
    "    'gene3': np.random.rand(100),\n",
    "    'gene4': np.random.rand(100),\n",
    "    'gene5': np.random.rand(100),\n",
    "    'gene6': np.random.rand(100),\n",
    "    'gene7': np.random.rand(100),\n",
    "    'gene8': np.random.rand(100),\n",
    "    'gene9': np.random.rand(100),\n",
    "    'gene10': np.random.rand(100),\n",
    "    'gene11': np.random.rand(100),\n",
    "    'gene12': np.random.rand(100),\n",
    "    'gene13': np.random.rand(100),\n",
    "    'gene14': np.random.rand(100),\n",
    "    'gene15': np.random.rand(100),\n",
    "    'gene16': np.random.rand(100),\n",
    "    'gene17': np.random.rand(100),\n",
    "    'gene18': np.random.rand(100),\n",
    "    'gene19': np.random.rand(100),\n",
    "    'gene20': np.random.rand(100),\n",
    "    'clinical1': np.random.randint(0, 2, size=100),\n",
    "    'clinical2': np.random.randint(0, 2, size=100),\n",
    "    'clinical3': np.random.randint(0, 5, size=100),\n",
    "    'Label': np.random.randint(0, 2, size=100),\n",
    "})\n",
    "unlabelled_data = pd.DataFrame({\n",
    "    'gene1': np.random.rand(500),\n",
    "    'gene2': np.random.rand(500),\n",
    "    'gene3': np.random.rand(500),\n",
    "    'gene4': np.random.rand(500),\n",
    "    'gene5': np.random.rand(500),\n",
    "    'gene6': np.random.rand(500),\n",
    "    'gene7': np.random.rand(500),\n",
    "    'gene8': np.random.rand(500),\n",
    "    'gene9': np.random.rand(500),\n",
    "    'gene10': np.random.rand(500),\n",
    "    'gene11': np.random.rand(500),\n",
    "    'gene12': np.random.rand(500),\n",
    "    'gene13': np.random.rand(500),\n",
    "    'gene14': np.random.rand(500),\n",
    "    'gene15': np.random.rand(500),\n",
    "    'gene16': np.random.rand(500),\n",
    "    'gene17': np.random.rand(500),\n",
    "    'gene18': np.random.rand(500),\n",
    "    'gene19': np.random.rand(500),\n",
    "    'gene20': np.random.rand(500),\n",
    "    'clinical1': np.random.randint(0, 2, size=500),\n",
    "    'clinical2': np.random.randint(0, 2, size=500),\n",
    "    'clinical3': np.random.randint(0, 5, size=500),\n",
    "})\n",
    "\n",
    "print(f'Train data shape: {train_data.shape}')\n",
    "print(f'Test data shape: {test_data.shape}')\n",
    "print(f'Unlabelled data shape: {unlabelled_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene1</th>\n",
       "      <th>gene2</th>\n",
       "      <th>gene3</th>\n",
       "      <th>gene4</th>\n",
       "      <th>gene5</th>\n",
       "      <th>gene6</th>\n",
       "      <th>gene7</th>\n",
       "      <th>gene8</th>\n",
       "      <th>gene9</th>\n",
       "      <th>gene10</th>\n",
       "      <th>...</th>\n",
       "      <th>gene15</th>\n",
       "      <th>gene16</th>\n",
       "      <th>gene17</th>\n",
       "      <th>gene18</th>\n",
       "      <th>gene19</th>\n",
       "      <th>gene20</th>\n",
       "      <th>clinical1</th>\n",
       "      <th>clinical2</th>\n",
       "      <th>clinical3</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.638455</td>\n",
       "      <td>0.712732</td>\n",
       "      <td>0.588723</td>\n",
       "      <td>0.426820</td>\n",
       "      <td>0.756832</td>\n",
       "      <td>0.383281</td>\n",
       "      <td>0.054584</td>\n",
       "      <td>0.750556</td>\n",
       "      <td>0.963073</td>\n",
       "      <td>0.620147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.801782</td>\n",
       "      <td>0.500313</td>\n",
       "      <td>0.846646</td>\n",
       "      <td>0.131793</td>\n",
       "      <td>0.640292</td>\n",
       "      <td>0.870246</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981859</td>\n",
       "      <td>0.850545</td>\n",
       "      <td>0.536948</td>\n",
       "      <td>0.294058</td>\n",
       "      <td>0.398688</td>\n",
       "      <td>0.174936</td>\n",
       "      <td>0.864578</td>\n",
       "      <td>0.188017</td>\n",
       "      <td>0.801485</td>\n",
       "      <td>0.943521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.814415</td>\n",
       "      <td>0.134850</td>\n",
       "      <td>0.830166</td>\n",
       "      <td>0.984613</td>\n",
       "      <td>0.325049</td>\n",
       "      <td>0.551947</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426209</td>\n",
       "      <td>0.633602</td>\n",
       "      <td>0.963568</td>\n",
       "      <td>0.016711</td>\n",
       "      <td>0.743181</td>\n",
       "      <td>0.190936</td>\n",
       "      <td>0.674415</td>\n",
       "      <td>0.498850</td>\n",
       "      <td>0.672768</td>\n",
       "      <td>0.679983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.978305</td>\n",
       "      <td>0.591408</td>\n",
       "      <td>0.871991</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.360970</td>\n",
       "      <td>0.557282</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.830004</td>\n",
       "      <td>0.496601</td>\n",
       "      <td>0.556880</td>\n",
       "      <td>0.517864</td>\n",
       "      <td>0.052628</td>\n",
       "      <td>0.867221</td>\n",
       "      <td>0.898235</td>\n",
       "      <td>0.823344</td>\n",
       "      <td>0.370235</td>\n",
       "      <td>0.177292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629143</td>\n",
       "      <td>0.925262</td>\n",
       "      <td>0.762207</td>\n",
       "      <td>0.492652</td>\n",
       "      <td>0.451114</td>\n",
       "      <td>0.085217</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.093481</td>\n",
       "      <td>0.005752</td>\n",
       "      <td>0.714529</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.519799</td>\n",
       "      <td>0.456049</td>\n",
       "      <td>0.189213</td>\n",
       "      <td>0.310339</td>\n",
       "      <td>0.068707</td>\n",
       "      <td>0.301711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809264</td>\n",
       "      <td>0.559337</td>\n",
       "      <td>0.273041</td>\n",
       "      <td>0.608062</td>\n",
       "      <td>0.837744</td>\n",
       "      <td>0.545713</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      gene1     gene2     gene3     gene4     gene5     gene6     gene7  \\\n",
       "0  0.638455  0.712732  0.588723  0.426820  0.756832  0.383281  0.054584   \n",
       "1  0.981859  0.850545  0.536948  0.294058  0.398688  0.174936  0.864578   \n",
       "2  0.426209  0.633602  0.963568  0.016711  0.743181  0.190936  0.674415   \n",
       "3  0.830004  0.496601  0.556880  0.517864  0.052628  0.867221  0.898235   \n",
       "4  0.093481  0.005752  0.714529  0.026670  0.519799  0.456049  0.189213   \n",
       "\n",
       "      gene8     gene9    gene10  ...    gene15    gene16    gene17    gene18  \\\n",
       "0  0.750556  0.963073  0.620147  ...  0.801782  0.500313  0.846646  0.131793   \n",
       "1  0.188017  0.801485  0.943521  ...  0.814415  0.134850  0.830166  0.984613   \n",
       "2  0.498850  0.672768  0.679983  ...  0.978305  0.591408  0.871991  0.004320   \n",
       "3  0.823344  0.370235  0.177292  ...  0.629143  0.925262  0.762207  0.492652   \n",
       "4  0.310339  0.068707  0.301711  ...  0.809264  0.559337  0.273041  0.608062   \n",
       "\n",
       "     gene19    gene20  clinical1  clinical2  clinical3  Label  \n",
       "0  0.640292  0.870246          0          0          4      0  \n",
       "1  0.325049  0.551947          1          1          0      1  \n",
       "2  0.360970  0.557282          0          1          1      0  \n",
       "3  0.451114  0.085217          1          0          2      1  \n",
       "4  0.837744  0.545713          1          0          4      1  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gene1',\n",
       " 'gene2',\n",
       " 'gene3',\n",
       " 'gene4',\n",
       " 'gene5',\n",
       " 'gene6',\n",
       " 'gene7',\n",
       " 'gene8',\n",
       " 'gene9',\n",
       " 'gene10',\n",
       " 'gene11',\n",
       " 'gene12',\n",
       " 'gene13',\n",
       " 'gene14',\n",
       " 'gene15',\n",
       " 'gene16',\n",
       " 'gene17',\n",
       " 'gene18',\n",
       " 'gene19',\n",
       " 'gene20']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols = test_data.drop(columns=['clinical1', 'clinical2', 'clinical3', 'Label']).columns.tolist()\n",
    "\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['clinical1', 'clinical2', 'clinical3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_cols) + len(numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_X_train = train_data.drop(columns=['Label'])\n",
    "full_y_train = train_data['Label']\n",
    "\n",
    "X_test = test_data.drop(columns=['Label'])\n",
    "y_test = test_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (80, 23)\n",
      "Validation data shape: (20, 23)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the train_data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    full_X_train,\n",
    "    full_y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=full_y_train)\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Validation data shape: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = \"accuracy_score\"\n",
    "input_dim = X_train.shape[1]\n",
    "predictor_dim = 1024\n",
    "alpha1 = 2.0\n",
    "alpha2 = 2.0\n",
    "beta = 1.0\n",
    "K = 2\n",
    "p_m = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_config = IdentityEmbeddingConfig(input_dim = input_dim)\n",
    "backbone_config = MLPBackboneConfig(input_dim = embedding_config.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 5]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardinality = get_category_cardinality(X_train, categorical_cols)\n",
    "\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = VIMEConfig( \n",
    "                    task=\"classification\",\n",
    "                    loss_fn=\"CrossEntropyLoss\",\n",
    "                    metric=metric,\n",
    "                    metric_hparams={},\n",
    "                    embedding_config=embedding_config,\n",
    "                    backbone_config=backbone_config,\n",
    "                    predictor_dim=predictor_dim,\n",
    "                    output_dim=2,\n",
    "                    alpha1=alpha1,\n",
    "                    alpha2=alpha2, \n",
    "                    beta=beta,\n",
    "                    K=K,\n",
    "                    p_m = p_m,\n",
    "                    cat_cardinality=get_category_cardinality(X_train, categorical_cols),\n",
    "                    num_continuous=len(numerical_cols),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl_vime = VIMELightning(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First Phase Learning\n",
    "train_ds = VIMEDataset(\n",
    "    X=X_train,\n",
    "    unlabeled_data=unlabelled_data,\n",
    "    config=config,\n",
    "    continuous_cols=numerical_cols,\n",
    "    category_cols=categorical_cols\n",
    ")\n",
    "\n",
    "valid_ds = VIMEDataset(\n",
    "    X=X_train,\n",
    "    config=config,\n",
    "    continuous_cols=numerical_cols,\n",
    "    category_cols=categorical_cols)\n",
    "\n",
    "datamodule = TS3LDataModule(train_ds, valid_ds, batch_size, train_sampler='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "2024-12-10 21:59:22.516735: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 21:59:22.526449: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 21:59:22.529367: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 21:59:22.537259: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 21:59:23.627112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name                        | Type             | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | task_loss_fn                | CrossEntropyLoss | 0      | train\n",
      "1 | mask_loss_fn                | BCELoss          | 0      | train\n",
      "2 | categorical_feature_loss_fn | CrossEntropyLoss | 0      | train\n",
      "3 | continuous_feature_loss_fn  | MSELoss          | 0      | train\n",
      "4 | consistency_loss_fn         | MSELoss          | 0      | train\n",
      "5 | model                       | VIME             | 1.2 M  | train\n",
      "-------------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.841     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 5/5 [00:01<00:00,  3.61it/s, v_num=14, train_loss=9.650, val_loss=7.120]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 5/5 [00:01<00:00,  3.50it/s, v_num=14, train_loss=9.650, val_loss=7.120]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "                    accelerator = 'cpu',\n",
    "                    max_epochs = max_epochs,\n",
    "                    num_sanity_val_steps = 2,\n",
    "    )\n",
    "\n",
    "trainer.fit(pl_vime, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts3l.utils.vime_utils import VIMESecondPhaseCollateFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_vime.set_second_phase()\n",
    "\n",
    "train_ds = VIMEDataset(\n",
    "    X_train,\n",
    "    y_train.values,\n",
    "    config,\n",
    "    unlabeled_data=unlabelled_data,\n",
    "    continuous_cols=numerical_cols,\n",
    "    category_cols=categorical_cols,\n",
    "    is_second_phase=True)\n",
    "\n",
    "valid_ds = VIMEDataset(\n",
    "    X_val,\n",
    "    y_val.values,\n",
    "    config,\n",
    "    continuous_cols=numerical_cols,\n",
    "    category_cols=categorical_cols,\n",
    "    is_second_phase=True)\n",
    "\n",
    "datamodule = TS3LDataModule(\n",
    "    train_ds,\n",
    "    valid_ds,\n",
    "    batch_size=batch_size,\n",
    "    train_sampler=\"weighted\",\n",
    "    train_collate_fn=VIMESecondPhaseCollateFN()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name                        | Type             | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | task_loss_fn                | CrossEntropyLoss | 0      | train\n",
      "1 | mask_loss_fn                | BCELoss          | 0      | train\n",
      "2 | categorical_feature_loss_fn | CrossEntropyLoss | 0      | train\n",
      "3 | continuous_feature_loss_fn  | MSELoss          | 0      | train\n",
      "4 | consistency_loss_fn         | MSELoss          | 0      | train\n",
      "5 | model                       | VIME             | 1.2 M  | train\n",
      "-------------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "19.8 K    Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.841     Total estimated model params size (MB)\n",
      "26        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.910, train_accuracy_score=0.530, val_accuracy_score=0.550, val_loss=1.040]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.550, train_accuracy_score=0.626, val_accuracy_score=0.400, val_loss=0.794]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.360, train_accuracy_score=0.643, val_accuracy_score=0.450, val_loss=0.737]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.320, train_accuracy_score=0.697, val_accuracy_score=0.600, val_loss=0.738]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.180, train_accuracy_score=0.700, val_accuracy_score=0.400, val_loss=0.756]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.110, train_accuracy_score=0.716, val_accuracy_score=0.400, val_loss=0.811]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.060, train_accuracy_score=0.783, val_accuracy_score=0.400, val_loss=0.815]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.090, train_accuracy_score=0.733, val_accuracy_score=0.450, val_loss=0.784]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.994, train_accuracy_score=0.783, val_accuracy_score=0.300, val_loss=0.768]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=1.000, train_accuracy_score=0.766, val_accuracy_score=0.350, val_loss=0.747]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.914, train_accuracy_score=0.796, val_accuracy_score=0.550, val_loss=0.731]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.908, train_accuracy_score=0.793, val_accuracy_score=0.500, val_loss=0.747]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.914, train_accuracy_score=0.826, val_accuracy_score=0.350, val_loss=0.740]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.845, train_accuracy_score=0.848, val_accuracy_score=0.450, val_loss=0.752]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.834, train_accuracy_score=0.873, val_accuracy_score=0.500, val_loss=0.777]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.901, train_accuracy_score=0.874, val_accuracy_score=0.450, val_loss=0.789]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.854, train_accuracy_score=0.866, val_accuracy_score=0.450, val_loss=0.791]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.814, train_accuracy_score=0.889, val_accuracy_score=0.450, val_loss=0.772]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:   0%|          | 0/5 [00:00<?, ?it/s, v_num=15, train_loss=0.773, train_accuracy_score=0.883, val_accuracy_score=0.550, val_loss=0.740]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 5/5 [00:01<00:00,  2.72it/s, v_num=15, train_loss=0.760, train_accuracy_score=0.906, val_accuracy_score=0.600, val_loss=0.726]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s, v_num=15, train_loss=0.760, train_accuracy_score=0.906, val_accuracy_score=0.600, val_loss=0.726]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "                    accelerator = 'cpu',\n",
    "                    max_epochs = max_epochs,\n",
    "                    num_sanity_val_steps = 2,\n",
    "    )\n",
    "\n",
    "trainer.fit(pl_vime, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = VIMEDataset(\n",
    "    X_test,\n",
    "    category_cols=categorical_cols,\n",
    "    continuous_cols=numerical_cols,\n",
    "    is_second_phase=True)\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(test_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 263.63it/s]\n",
      "Accuracy 0.61\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(pl_vime, test_dl)\n",
    "preds = F.softmax(torch.concat([out.cpu() for out in preds]).squeeze(),dim=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, preds.argmax(1))\n",
    "\n",
    "print(\"Accuracy %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "        0, 1, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels = preds.argmax(1)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5715, 0.5717, 0.6309, 0.6341, 0.5809, 0.8216, 0.5969, 0.5610, 0.7469,\n",
       "        0.5396, 0.6850, 0.8219, 0.6587, 0.5450, 0.6681, 0.6714, 0.6885, 0.5077,\n",
       "        0.5143, 0.6331, 0.5836, 0.6161, 0.6325, 0.5270, 0.5828, 0.7477, 0.5748,\n",
       "        0.5436, 0.5346, 0.6640, 0.5476, 0.7793, 0.5039, 0.6830, 0.5082, 0.5235,\n",
       "        0.7137, 0.6819, 0.7577, 0.6653, 0.6044, 0.7446, 0.7338, 0.6873, 0.6213,\n",
       "        0.6107, 0.7259, 0.7185, 0.5838, 0.5615, 0.5143, 0.6035, 0.5953, 0.7192,\n",
       "        0.6555, 0.6455, 0.6732, 0.5580, 0.5787, 0.5589, 0.5007, 0.6046, 0.5825,\n",
       "        0.5714, 0.5279, 0.5690, 0.6658, 0.5034, 0.5061, 0.5915, 0.5772, 0.5398,\n",
       "        0.6502, 0.5289, 0.7316, 0.6713, 0.6589, 0.6262, 0.5151, 0.6431, 0.7293,\n",
       "        0.5678, 0.5087, 0.5020, 0.5977, 0.6561, 0.5102, 0.5638, 0.6500, 0.5910,\n",
       "        0.7827, 0.7057, 0.6845, 0.5859, 0.5732, 0.6950, 0.6099, 0.5907, 0.7664,\n",
       "        0.5807])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "associated_probabilities = preds[np.arange(preds.shape[0]), predicted_labels]\n",
    "\n",
    "associated_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.571475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.571686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.630942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.634131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0</td>\n",
       "      <td>0.694961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>0.609941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>0.590713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>0.766423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>0.580741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  probability\n",
       "0       1     0.571475\n",
       "1       1     0.571686\n",
       "2       0     0.630942\n",
       "3       1     0.634131\n",
       "4       1     0.580900\n",
       "..    ...          ...\n",
       "95      0     0.694961\n",
       "96      0     0.609941\n",
       "97      1     0.590713\n",
       "98      1     0.766423\n",
       "99      0     0.580741\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'label': predicted_labels, 'probability': associated_probabilities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_ds = VIMEDataset(\n",
    "    X=X_train,\n",
    "    config=config,\n",
    "    continuous_cols=numerical_cols,\n",
    "    category_cols=categorical_cols)\n",
    "\n",
    "unlabelled_dl = DataLoader(\n",
    "    unlabelled_ds,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=SequentialSampler(unlabelled_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/home/sonk/envs/pandas/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 182.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.663603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.680838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.850472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.748557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.591080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0</td>\n",
       "      <td>0.656666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0</td>\n",
       "      <td>0.649640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0.528778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>0.503200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0</td>\n",
       "      <td>0.701999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label  confidence\n",
       "0       1    0.663603\n",
       "1       1    0.680838\n",
       "2       1    0.850472\n",
       "3       1    0.748557\n",
       "4       1    0.591080\n",
       "..    ...         ...\n",
       "75      0    0.656666\n",
       "76      0    0.649640\n",
       "77      0    0.528778\n",
       "78      0    0.503200\n",
       "79      0    0.701999\n",
       "\n",
       "[80 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = trainer.predict(pl_vime, unlabelled_dl)\n",
    "preds = F.softmax(torch.concat([out.cpu() for out in preds]).squeeze(),dim=1)\n",
    "\n",
    "predicted_labels = preds.argmax(1)\n",
    "associated_probabilities = preds[np.arange(preds.shape[0]), predicted_labels]\n",
    "\n",
    "pd.DataFrame({'label': predicted_labels, 'confidence': associated_probabilities})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
