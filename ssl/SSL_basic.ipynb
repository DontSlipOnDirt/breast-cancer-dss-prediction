{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import"
      ],
      "metadata": {
        "id": "Lb1gsFNTbxac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lifelines\n",
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdexOscBbwsY",
        "outputId": "3c0f9ee4-74c5-436c-ce91-2368239d6e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lifelines in /usr/local/lib/python3.10/dist-packages (0.30.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.1 in /usr/local/lib/python3.10/dist-packages (from lifelines) (2.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from lifelines) (3.8.0)\n",
            "Requirement already satisfied: autograd>=1.5 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.7.0)\n",
            "Requirement already satisfied: autograd-gamma>=0.3 in /usr/local/lib/python3.10/dist-packages (from lifelines) (0.5.0)\n",
            "Requirement already satisfied: formulaic>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from lifelines) (1.0.2)\n",
            "Requirement already satisfied: interface-meta>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.0 in /usr/local/lib/python3.10/dist-packages (from formulaic>=0.2.2->lifelines) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->lifelines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.1->lifelines) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->lifelines) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data"
      ],
      "metadata": {
        "id": "zqwswCTQ8JwT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74YfKoxEuo9R",
        "outputId": "4e8025a3-1d22-46c9-de89-ca355b959e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# connect to goodle drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # to read files from your google drive\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks') # to import our custom module from this path later\n",
        "\n",
        "read_path = '/content/drive/My Drive/AIIM/Final/' # modify this line according to your path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read the npz file\n",
        "import numpy as np\n",
        "\n",
        "data_l = np.load(os.path.join(read_path, \"breast_all.npz\"), allow_pickle=True)\n",
        "data_ul = np.load(os.path.join(read_path, \"breast_unlabeled.npz\"), allow_pickle=True)\n",
        "\n",
        "print(\"Keys: \", data_l.files, '\\n')\n",
        "print(\"Keys: \", data_ul.files, '\\n')\n",
        "\n",
        "train_data_path = read_path + \"train_data.csv\"\n",
        "test_data_path = read_path + \"test_data.csv\"\n",
        "\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJlhzOBzszH",
        "outputId": "8972fb71-6862-4a9d-e2d9-2b476372bfd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys:  ['x_train', 'c_train', 'y_train', 'o_train', 'e_train', 'x_test', 'c_test', 'y_test', 'o_test', 'e_test', 'gene_name', 'clinical_feature'] \n",
            "\n",
            "Keys:  ['x_w_full', 'c_w_full', 'x_n_full', 'c_n_full'] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# labeled data\n",
        "x_train = data_l[\"x_train\"] # genetic data\n",
        "c_train = data_l[\"c_train\"] # clinical data\n",
        "y_train = data_l[\"y_train\"]\n",
        "\n",
        "x_test = data_l[\"x_test\"]\n",
        "c_test = data_l[\"c_test\"]\n",
        "y_test = data_l[\"y_test\"]\n",
        "\n",
        "# unlabeled data\n",
        "x_ul = data_ul[\"x_w_full\"]\n",
        "c_ul = data_ul[\"c_w_full\"]\n",
        "\n",
        "# info\n",
        "gene_name = data_l[\"gene_name\"]\n",
        "clinical_feature = data_l[\"clinical_feature\"]"
      ],
      "metadata": {
        "id": "nSnSxvqS0Q-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Semi-Supervised Learning (SSL)"
      ],
      "metadata": {
        "id": "FePUSut2YYdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Basic SSL with 10 Loops Limitation"
      ],
      "metadata": {
        "id": "oslRddrUaOGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**\n",
        "```\n",
        "base_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "```\n",
        "\n",
        "**Explination**\n",
        "- `(\"scaler\", StandardScaler())`\n",
        "    \n",
        "    Standardizes the features\n",
        "\n",
        "- `(\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))`\n",
        "\n",
        "  - Random Forest Classifier\n",
        "  - `n_estimators=100`: Specifies the number of decision trees in the forest.\n",
        "  - `random_state=42`: Ensures reproducibility of results by fixing the random seed.\n"
      ],
      "metadata": {
        "id": "Uu_XeYc9gLQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Combine gene expression and clinical data\n",
        "def combine_features(gene_data, clinical_data):\n",
        "    return np.hstack((gene_data, clinical_data))\n",
        "\n",
        "# Convert labels from True/False to 1/0\n",
        "y_train = np.array([1 if label else 0 for label in y_train])\n",
        "\n",
        "# Combine labeled data\n",
        "X = combine_features(x_train, c_train)\n",
        "y = y_train\n",
        "\n",
        "# Split labeled data into a training set and a test set\n",
        "X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create a pipeline with scaling and a classifier\n",
        "base_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Self-training\n",
        "max_iterations = 10\n",
        "confidence_threshold = 0.9\n",
        "X_labeled = X_train.copy()\n",
        "y_labeled = y_train_split.copy()\n",
        "X_unlabeled = combine_features(x_ul, c_ul)\n",
        "pseudo_labels = []\n",
        "\n",
        "for iteration in range(max_iterations):\n",
        "    print(f\"Iteration {iteration + 1}...\")\n",
        "\n",
        "    # Train the model on the labeled dataset\n",
        "    base_model.fit(X_labeled, y_labeled)\n",
        "\n",
        "    # Predict probabilities on the unlabeled dataset\n",
        "    probs = base_model.predict_proba(X_unlabeled)\n",
        "    pseudo_labels = np.argmax(probs, axis=1)  # Predicted labels\n",
        "    confidence_scores = np.max(probs, axis=1)  # Max probabilities\n",
        "\n",
        "    # Select confident predictions\n",
        "    confident_indices = np.where(confidence_scores >= confidence_threshold)[0]\n",
        "    if len(confident_indices) == 0:\n",
        "        print(\"No confident predictions in this iteration. Stopping...\")\n",
        "        break\n",
        "\n",
        "    # Add confident predictions to the labeled dataset\n",
        "    X_labeled = np.vstack((X_labeled, X_unlabeled[confident_indices]))\n",
        "    y_labeled = np.hstack((y_labeled, pseudo_labels[confident_indices]))\n",
        "    X_unlabeled = np.delete(X_unlabeled, confident_indices, axis=0)\n",
        "\n",
        "    print(f\"Added {len(confident_indices)} pseudo-labeled samples.\")\n",
        "\n",
        "# Evaluate on split test set\n",
        "y_test_pred = base_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test_split, y_test_pred)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Combine original and pseudo-labeled data\n",
        "labeled_data = pd.DataFrame(np.hstack((X_train, y_train_split.reshape(-1, 1))),\n",
        "                            columns=[*gene_name, *clinical_feature, 'Label'])\n",
        "\n",
        "pseudo_labeled_data = pd.DataFrame(\n",
        "    np.hstack((X_labeled[len(X_train):], y_labeled[len(X_train):].reshape(-1, 1))),\n",
        "    columns=[*gene_name, *clinical_feature, 'Label']\n",
        ")\n",
        "\n",
        "# Ensure labels are stored as 0/1\n",
        "labeled_data['Label'] = labeled_data['Label'].astype(int)\n",
        "pseudo_labeled_data['Label'] = pseudo_labeled_data['Label'].astype(int)\n",
        "\n",
        "combined_data = pd.concat([labeled_data, pseudo_labeled_data], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/content/drive/My Drive/AIIM/Final/combined_labeled_data.csv\"\n",
        "combined_data.to_csv(output_file, index=False)\n",
        "print(f\"Combined labeled data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrkGZ_H3t9fQ",
        "outputId": "87d55ac1-b045-4a4f-8a84-2797175b17e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1...\n",
            "Added 18 pseudo-labeled samples.\n",
            "Iteration 2...\n",
            "Added 15 pseudo-labeled samples.\n",
            "Iteration 3...\n",
            "Added 13 pseudo-labeled samples.\n",
            "Iteration 4...\n",
            "Added 14 pseudo-labeled samples.\n",
            "Iteration 5...\n",
            "Added 21 pseudo-labeled samples.\n",
            "Iteration 6...\n",
            "Added 25 pseudo-labeled samples.\n",
            "Iteration 7...\n",
            "Added 29 pseudo-labeled samples.\n",
            "Iteration 8...\n",
            "Added 33 pseudo-labeled samples.\n",
            "Iteration 9...\n",
            "Added 25 pseudo-labeled samples.\n",
            "Iteration 10...\n",
            "Added 16 pseudo-labeled samples.\n",
            "Test Accuracy: 0.6989\n",
            "Combined labeled data saved to /content/drive/My Drive/AIIM/Final/combined_labeled_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Basic SSL with No Loop Limitation (USE THIS ONE)"
      ],
      "metadata": {
        "id": "cMPrjUfiaTjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**\n",
        "```\n",
        "base_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "```\n",
        "\n",
        "**Explination**\n",
        "- `(\"scaler\", StandardScaler())`\n",
        "    \n",
        "    Standardizes the features\n",
        "\n",
        "- `(\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))`\n",
        "\n",
        "  - Random Forest Classifier\n",
        "  - `n_estimators=100`: Specifies the number of decision trees in the forest.\n",
        "  - `random_state=42`: Ensures reproducibility of results by fixing the random seed.\n"
      ],
      "metadata": {
        "id": "da6umdQSePV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Combine gene expression and clinical data\n",
        "def combine_features(gene_data, clinical_data):\n",
        "    return np.hstack((gene_data, clinical_data))\n",
        "\n",
        "# Convert labels from True/False to 1/0\n",
        "y_train = np.array([1 if label else 0 for label in y_train])\n",
        "\n",
        "# Combine labeled data\n",
        "X = combine_features(x_train, c_train)\n",
        "y = y_train\n",
        "\n",
        "# Split labeled data into a training set and a test set\n",
        "X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Create a pipeline with scaling and a classifier\n",
        "base_model = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "# Self-training\n",
        "confidence_threshold = 0.9\n",
        "X_labeled = X_train.copy()\n",
        "y_labeled = y_train_split.copy()\n",
        "X_unlabeled = combine_features(x_ul, c_ul)\n",
        "confidence_labeled = np.ones(len(y_labeled))  # Confidence for labeled data is 1\n",
        "pseudo_labels = []\n",
        "confidence_scores_pseudo = []\n",
        "\n",
        "total_added = 0  # Counter to track total number of pseudo-labeled data added\n",
        "\n",
        "while True:\n",
        "    print(\"Starting new iteration...\")\n",
        "\n",
        "    # Train the model on the labeled dataset\n",
        "    base_model.fit(X_labeled, y_labeled)\n",
        "\n",
        "    # Predict probabilities on the unlabeled dataset\n",
        "    probs = base_model.predict_proba(X_unlabeled)\n",
        "    pseudo_labels = np.argmax(probs, axis=1)  # Predicted labels\n",
        "    confidence_scores = np.max(probs, axis=1)  # Max probabilities\n",
        "\n",
        "    # Select confident predictions\n",
        "    confident_indices = np.where(confidence_scores >= confidence_threshold)[0]\n",
        "    if len(confident_indices) == 0:\n",
        "        print(\"No confident predictions left. Stopping...\")\n",
        "        break\n",
        "\n",
        "    # Add confident predictions to the labeled dataset\n",
        "    X_labeled = np.vstack((X_labeled, X_unlabeled[confident_indices]))\n",
        "    y_labeled = np.hstack((y_labeled, pseudo_labels[confident_indices]))\n",
        "    confidence_labeled = np.hstack((confidence_labeled, confidence_scores[confident_indices]))\n",
        "    X_unlabeled = np.delete(X_unlabeled, confident_indices, axis=0)\n",
        "\n",
        "    total_added += len(confident_indices)  # Update the total added counter\n",
        "\n",
        "    print(f\"Added {len(confident_indices)} pseudo-labeled samples.\")\n",
        "\n",
        "# Output the total number of pseudo-labeled data added\n",
        "print(f\"Total pseudo-labeled data added: {total_added}\")\n",
        "\n",
        "# Combine original and pseudo-labeled data\n",
        "labeled_data = pd.DataFrame(np.hstack((X_train, y_train_split.reshape(-1, 1), np.ones((len(y_train_split), 1)))),\n",
        "                            columns=[*gene_name, *clinical_feature, 'Label', 'Confidence'])\n",
        "\n",
        "pseudo_labeled_data = pd.DataFrame(\n",
        "    np.hstack((X_labeled[len(X_train):], y_labeled[len(X_train):].reshape(-1, 1), confidence_labeled[len(X_train):].reshape(-1, 1))),\n",
        "    columns=[*gene_name, *clinical_feature, 'Label', 'Confidence']\n",
        ")\n",
        "\n",
        "# Ensure labels are stored as 0/1\n",
        "labeled_data['Label'] = labeled_data['Label'].astype(int)\n",
        "pseudo_labeled_data['Label'] = pseudo_labeled_data['Label'].astype(int)\n",
        "\n",
        "combined_data = pd.concat([labeled_data, pseudo_labeled_data], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/content/drive/My Drive/AIIM/Final/combined_labeled_data_w_confidence.csv\"\n",
        "combined_data.to_csv(output_file, index=False)\n",
        "print(f\"Combined labeled data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7QBCrHcaLsm",
        "outputId": "072b495c-e5a5-4dd9-dc6e-2bd7ce584d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting new iteration...\n",
            "Added 18 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 15 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 13 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 14 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 21 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 25 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 29 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 33 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 25 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 16 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 17 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 19 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 9 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 14 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 10 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 6 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 7 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 5 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 5 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 6 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 5 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 8 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 3 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 8 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 5 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 10 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 6 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 11 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 13 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 10 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 8 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 9 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 1 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 2 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "No confident predictions left. Stopping...\n",
            "Total pseudo-labeled data added: 426\n",
            "Combined labeled data saved to /content/drive/My Drive/AIIM/Final/combined_labeled_data_w_confidence.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting - XGBoost"
      ],
      "metadata": {
        "id": "HA92ReeSesxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- with Hyperparameter Tuning\n",
        "- the result is terrible haha\n",
        "- dump!"
      ],
      "metadata": {
        "id": "swZNZ02K4sM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Combine gene expression and clinical data\n",
        "def combine_features(gene_data, clinical_data):\n",
        "    return np.hstack((gene_data, clinical_data))\n",
        "\n",
        "# Convert labels from True/False to 1/0\n",
        "y_train = np.array([1 if label else 0 for label in y_train])\n",
        "\n",
        "# Combine labeled data\n",
        "X = combine_features(x_train, c_train)\n",
        "y = y_train\n",
        "\n",
        "# Split labeled data into a training set and a test set\n",
        "X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Prepare the unlabeled data\n",
        "X_unlabeled = combine_features(x_ul, c_ul)\n",
        "X_unlabeled = scaler.transform(X_unlabeled)\n",
        "\n",
        "# Define the hyperparameter grid for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'n_estimators': randint(100, 500),               # Number of boosting rounds\n",
        "    'max_depth': randint(3, 10),                      # Depth of trees\n",
        "    'learning_rate': uniform(1e-3, 1e-1),              # Learning rate\n",
        "    'subsample': uniform(0.6, 0.2),                   # Subsample ratio\n",
        "    'colsample_bytree': uniform(0.6, 0.2),            # Column sampling\n",
        "    'scale_pos_weight': uniform(1, 10),               # Class imbalance weight\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "model = XGBClassifier(\n",
        "    # use_label_encoder=False,\n",
        "    eval_metric=\"logloss\"\n",
        ")\n",
        "\n",
        "# Set up RandomizedSearchCV with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,  # Number of parameter settings to try\n",
        "    scoring='accuracy',\n",
        "    cv=3,        # 3-fold cross-validation\n",
        "    random_state=42,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_train, y_train_split)\n",
        "\n",
        "# Get the best parameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Output the best parameters\n",
        "print(\"Best parameters found: \", best_params)\n",
        "\n",
        "# Self-training\n",
        "confidence_threshold = 0.9\n",
        "X_labeled = X_train.copy()\n",
        "y_labeled = y_train_split.copy()\n",
        "confidence_labeled = np.ones(len(y_labeled))  # Confidence for labeled data is 1\n",
        "pseudo_labels = []\n",
        "confidence_scores_pseudo = []\n",
        "\n",
        "total_added = 0  # Counter to track total number of pseudo-labeled data added\n",
        "\n",
        "while True:\n",
        "    print(\"Starting new iteration...\")\n",
        "\n",
        "    # Train the best model on the labeled dataset\n",
        "    best_model.fit(X_labeled, y_labeled)\n",
        "\n",
        "    # Predict probabilities on the unlabeled dataset\n",
        "    probs = best_model.predict_proba(X_unlabeled)\n",
        "    pseudo_labels = np.argmax(probs, axis=1)  # Predicted labels\n",
        "    confidence_scores = np.max(probs, axis=1)  # Max probabilities\n",
        "\n",
        "    # Select confident predictions\n",
        "    confident_indices = np.where(confidence_scores >= confidence_threshold)[0]\n",
        "    if len(confident_indices) == 0:\n",
        "        print(\"No confident predictions left. Stopping...\")\n",
        "        break\n",
        "\n",
        "    # Add confident predictions to the labeled dataset\n",
        "    X_labeled = np.vstack((X_labeled, X_unlabeled[confident_indices]))\n",
        "    y_labeled = np.hstack((y_labeled, pseudo_labels[confident_indices]))\n",
        "    confidence_labeled = np.hstack((confidence_labeled, confidence_scores[confident_indices]))\n",
        "    X_unlabeled = np.delete(X_unlabeled, confident_indices, axis=0)\n",
        "\n",
        "    total_added += len(confident_indices)  # Update the total added counter\n",
        "\n",
        "    print(f\"Added {len(confident_indices)} pseudo-labeled samples.\")\n",
        "\n",
        "# Output the total number of pseudo-labeled data added\n",
        "print(f\"Total pseudo-labeled data added: {total_added}\")\n",
        "\n",
        "# Combine original and pseudo-labeled data\n",
        "labeled_data = pd.DataFrame(np.hstack((X_train, y_train_split.reshape(-1, 1), np.ones((len(y_train_split), 1)))),\n",
        "                            columns=[*gene_name, *clinical_feature, 'Label', 'Confidence'])\n",
        "\n",
        "pseudo_labeled_data = pd.DataFrame(\n",
        "    np.hstack((X_labeled[len(X_train):], y_labeled[len(X_train):].reshape(-1, 1), confidence_labeled[len(X_train):].reshape(-1, 1))),\n",
        "    columns=[*gene_name, *clinical_feature, 'Label', 'Confidence']\n",
        ")\n",
        "\n",
        "# Ensure labels are stored as 0/1\n",
        "labeled_data['Label'] = labeled_data['Label'].astype(int)\n",
        "pseudo_labeled_data['Label'] = pseudo_labeled_data['Label'].astype(int)\n",
        "\n",
        "combined_data = pd.concat([labeled_data, pseudo_labeled_data], ignore_index=True)\n",
        "\n",
        "# Save to CSV\n",
        "output_file = \"/content/drive/My Drive/AIIM/Final/combined_labeled_data_xgboost_tuned.csv\"\n",
        "combined_data.to_csv(output_file, index=False)\n",
        "print(f\"Combined labeled data saved to {output_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYi3XoP0eqp2",
        "outputId": "218ef073-1da1-4e44-f437-09f199c7c03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
            "[CV] END colsample_bytree=0.6749080237694725, learning_rate=0.09607143064099162, max_depth=5, n_estimators=171, scale_pos_weight=6.986584841970366, subsample=0.6312037280884872; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6749080237694725, learning_rate=0.09607143064099162, max_depth=5, n_estimators=171, scale_pos_weight=6.986584841970366, subsample=0.6312037280884872; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.6749080237694725, learning_rate=0.09607143064099162, max_depth=5, n_estimators=171, scale_pos_weight=6.986584841970366, subsample=0.6312037280884872; total time=   3.2s\n",
            "[CV] END colsample_bytree=0.6311989040672406, learning_rate=0.006808361216819946, max_depth=7, n_estimators=199, scale_pos_weight=2.428668179219408, subsample=0.7301776945897706; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6311989040672406, learning_rate=0.006808361216819946, max_depth=7, n_estimators=199, scale_pos_weight=2.428668179219408, subsample=0.7301776945897706; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6311989040672406, learning_rate=0.006808361216819946, max_depth=7, n_estimators=199, scale_pos_weight=2.428668179219408, subsample=0.7301776945897706; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.61128231580542, learning_rate=0.07319987722668247, max_depth=8, n_estimators=393, scale_pos_weight=1.0077876584101433, subsample=0.7984423118582435; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.61128231580542, learning_rate=0.07319987722668247, max_depth=8, n_estimators=393, scale_pos_weight=1.0077876584101433, subsample=0.7984423118582435; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.61128231580542, learning_rate=0.07319987722668247, max_depth=8, n_estimators=393, scale_pos_weight=1.0077876584101433, subsample=0.7984423118582435; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7234963019255433, learning_rate=0.06216531604882809, max_depth=7, n_estimators=335, scale_pos_weight=1.2306242504141576, subsample=0.7049549320516778; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7234963019255433, learning_rate=0.06216531604882809, max_depth=7, n_estimators=335, scale_pos_weight=1.2306242504141576, subsample=0.7049549320516778; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7234963019255433, learning_rate=0.06216531604882809, max_depth=7, n_estimators=335, scale_pos_weight=1.2306242504141576, subsample=0.7049549320516778; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6799721943430511, learning_rate=0.005666566321361543, max_depth=6, n_estimators=370, scale_pos_weight=5.56069984217036, subsample=0.7570351922786027; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.6799721943430511, learning_rate=0.005666566321361543, max_depth=6, n_estimators=370, scale_pos_weight=5.56069984217036, subsample=0.7570351922786027; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.6799721943430511, learning_rate=0.005666566321361543, max_depth=6, n_estimators=370, scale_pos_weight=5.56069984217036, subsample=0.7570351922786027; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.639934756431672, learning_rate=0.05242344384136116, max_depth=3, n_estimators=230, scale_pos_weight=9.599404067363206, subsample=0.736061507717556; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.639934756431672, learning_rate=0.05242344384136116, max_depth=3, n_estimators=230, scale_pos_weight=9.599404067363206, subsample=0.736061507717556; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.639934756431672, learning_rate=0.05242344384136116, max_depth=3, n_estimators=230, scale_pos_weight=9.599404067363206, subsample=0.736061507717556; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.6900998503939086, learning_rate=0.002326496115986653, max_depth=3, n_estimators=415, scale_pos_weight=6.632882178455393, subsample=0.6770833005079833; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6900998503939086, learning_rate=0.002326496115986653, max_depth=3, n_estimators=415, scale_pos_weight=6.632882178455393, subsample=0.6770833005079833; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6900998503939086, learning_rate=0.002326496115986653, max_depth=3, n_estimators=415, scale_pos_weight=6.632882178455393, subsample=0.6770833005079833; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6031932504440428, learning_rate=0.024089382562214903, max_depth=6, n_estimators=466, scale_pos_weight=7.832635188254582, subsample=0.7219993315565242; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6031932504440428, learning_rate=0.024089382562214903, max_depth=6, n_estimators=466, scale_pos_weight=7.832635188254582, subsample=0.7219993315565242; total time=   2.3s\n",
            "[CV] END colsample_bytree=0.6031932504440428, learning_rate=0.024089382562214903, max_depth=6, n_estimators=466, scale_pos_weight=7.832635188254582, subsample=0.7219993315565242; total time=   3.7s\n",
            "[CV] END colsample_bytree=0.7666389823472328, learning_rate=0.01833646535077721, max_depth=3, n_estimators=149, scale_pos_weight=7.62522284353982, subsample=0.6623422152178822; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.7666389823472328, learning_rate=0.01833646535077721, max_depth=3, n_estimators=149, scale_pos_weight=7.62522284353982, subsample=0.6623422152178822; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.7666389823472328, learning_rate=0.01833646535077721, max_depth=3, n_estimators=149, scale_pos_weight=7.62522284353982, subsample=0.6623422152178822; total time=   0.1s\n",
            "[CV] END colsample_bytree=0.7040136042355621, learning_rate=0.05567102793432797, max_depth=8, n_estimators=290, scale_pos_weight=9.422847745949985, subsample=0.6899508266739531; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7040136042355621, learning_rate=0.05567102793432797, max_depth=8, n_estimators=290, scale_pos_weight=9.422847745949985, subsample=0.6899508266739531; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7040136042355621, learning_rate=0.05567102793432797, max_depth=8, n_estimators=290, scale_pos_weight=9.422847745949985, subsample=0.6899508266739531; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6790300472003629, learning_rate=0.09366588657937942, max_depth=9, n_estimators=403, scale_pos_weight=4.265407688058354, subsample=0.7140887948810799; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6790300472003629, learning_rate=0.09366588657937942, max_depth=9, n_estimators=403, scale_pos_weight=4.265407688058354, subsample=0.7140887948810799; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6790300472003629, learning_rate=0.09366588657937942, max_depth=9, n_estimators=403, scale_pos_weight=4.265407688058354, subsample=0.7140887948810799; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7041668520051647, learning_rate=0.09711720243493492, max_depth=7, n_estimators=307, scale_pos_weight=8.473201101373808, subsample=0.7079384264778159; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7041668520051647, learning_rate=0.09711720243493492, max_depth=7, n_estimators=307, scale_pos_weight=8.473201101373808, subsample=0.7079384264778159; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7041668520051647, learning_rate=0.09711720243493492, max_depth=7, n_estimators=307, scale_pos_weight=8.473201101373808, subsample=0.7079384264778159; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7173502331327697, learning_rate=0.0975255307264138, max_depth=6, n_estimators=479, scale_pos_weight=3.7599918202254337, subsample=0.6592547011408164; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7173502331327697, learning_rate=0.0975255307264138, max_depth=6, n_estimators=479, scale_pos_weight=3.7599918202254337, subsample=0.6592547011408164; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7173502331327697, learning_rate=0.0975255307264138, max_depth=6, n_estimators=479, scale_pos_weight=3.7599918202254337, subsample=0.6592547011408164; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6330533878126005, learning_rate=0.002563640674119393, max_depth=3, n_estimators=443, scale_pos_weight=4.948815181755697, subsample=0.6586976349436076; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6330533878126005, learning_rate=0.002563640674119393, max_depth=3, n_estimators=443, scale_pos_weight=4.948815181755697, subsample=0.6586976349436076; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6330533878126005, learning_rate=0.002563640674119393, max_depth=3, n_estimators=443, scale_pos_weight=4.948815181755697, subsample=0.6586976349436076; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6028159645430169, learning_rate=0.020884240408880518, max_depth=5, n_estimators=388, scale_pos_weight=7.059599747810114, subsample=0.7852601757026698; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.6028159645430169, learning_rate=0.020884240408880518, max_depth=5, n_estimators=388, scale_pos_weight=7.059599747810114, subsample=0.7852601757026698; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.6028159645430169, learning_rate=0.020884240408880518, max_depth=5, n_estimators=388, scale_pos_weight=7.059599747810114, subsample=0.7852601757026698; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7302154051003888, learning_rate=0.09249596755437808, max_depth=3, n_estimators=427, scale_pos_weight=5.494506741382034, subsample=0.6190820232980823; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7302154051003888, learning_rate=0.09249596755437808, max_depth=3, n_estimators=427, scale_pos_weight=5.494506741382034, subsample=0.6190820232980823; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7302154051003888, learning_rate=0.09249596755437808, max_depth=3, n_estimators=427, scale_pos_weight=5.494506741382034, subsample=0.6190820232980823; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6741636504396532, learning_rate=0.06788412526636073, max_depth=7, n_estimators=198, scale_pos_weight=6.912977877077271, subsample=0.6549443585980128; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.6741636504396532, learning_rate=0.06788412526636073, max_depth=7, n_estimators=198, scale_pos_weight=6.912977877077271, subsample=0.6549443585980128; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.6741636504396532, learning_rate=0.06788412526636073, max_depth=7, n_estimators=198, scale_pos_weight=6.912977877077271, subsample=0.6549443585980128; total time=   1.2s\n",
            "[CV] END colsample_bytree=0.7122486851695402, learning_rate=0.039292687475378986, max_depth=9, n_estimators=230, scale_pos_weight=8.607850486168974, subsample=0.7122554395138992; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7122486851695402, learning_rate=0.039292687475378986, max_depth=9, n_estimators=230, scale_pos_weight=8.607850486168974, subsample=0.7122554395138992; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7122486851695402, learning_rate=0.039292687475378986, max_depth=9, n_estimators=230, scale_pos_weight=8.607850486168974, subsample=0.7122554395138992; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7541934359909122, learning_rate=0.05037955963643908, max_depth=3, n_estimators=306, scale_pos_weight=5.275410183585496, subsample=0.605083825348819; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7541934359909122, learning_rate=0.05037955963643908, max_depth=3, n_estimators=306, scale_pos_weight=5.275410183585496, subsample=0.605083825348819; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7541934359909122, learning_rate=0.05037955963643908, max_depth=3, n_estimators=306, scale_pos_weight=5.275410183585496, subsample=0.605083825348819; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6215782853986609, learning_rate=0.004142918568673425, max_depth=9, n_estimators=340, scale_pos_weight=4.143559810763267, subsample=0.7017141382329406; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.6215782853986609, learning_rate=0.004142918568673425, max_depth=9, n_estimators=340, scale_pos_weight=4.143559810763267, subsample=0.7017141382329406; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.6215782853986609, learning_rate=0.004142918568673425, max_depth=9, n_estimators=340, scale_pos_weight=4.143559810763267, subsample=0.7017141382329406; total time=   1.0s\n",
            "[CV] END colsample_bytree=0.7815132947852186, learning_rate=0.025929222914887497, max_depth=7, n_estimators=242, scale_pos_weight=8.555511385430487, subsample=0.6457596330983245; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7815132947852186, learning_rate=0.025929222914887497, max_depth=7, n_estimators=242, scale_pos_weight=8.555511385430487, subsample=0.6457596330983245; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7815132947852186, learning_rate=0.025929222914887497, max_depth=7, n_estimators=242, scale_pos_weight=8.555511385430487, subsample=0.6457596330983245; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.6153959819657586, learning_rate=0.029975145291376806, max_depth=8, n_estimators=383, scale_pos_weight=10.29697652342573, subsample=0.7616240759128834; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6153959819657586, learning_rate=0.029975145291376806, max_depth=8, n_estimators=383, scale_pos_weight=10.29697652342573, subsample=0.7616240759128834; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6153959819657586, learning_rate=0.029975145291376806, max_depth=8, n_estimators=383, scale_pos_weight=10.29697652342573, subsample=0.7616240759128834; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7266807513020846, learning_rate=0.08814605901877177, max_depth=6, n_estimators=207, scale_pos_weight=2.865700588860358, subsample=0.7785117996979956; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7266807513020846, learning_rate=0.08814605901877177, max_depth=6, n_estimators=207, scale_pos_weight=2.865700588860358, subsample=0.7785117996979956; total time=   1.1s\n",
            "[CV] END colsample_bytree=0.7266807513020846, learning_rate=0.08814605901877177, max_depth=6, n_estimators=207, scale_pos_weight=2.865700588860358, subsample=0.7785117996979956; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.7078684483831301, learning_rate=0.08174401551640625, max_depth=6, n_estimators=330, scale_pos_weight=10.06828441545754, subsample=0.654426449876927; total time=   1.8s\n",
            "[CV] END colsample_bytree=0.7078684483831301, learning_rate=0.08174401551640625, max_depth=6, n_estimators=330, scale_pos_weight=10.06828441545754, subsample=0.654426449876927; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7078684483831301, learning_rate=0.08174401551640625, max_depth=6, n_estimators=330, scale_pos_weight=10.06828441545754, subsample=0.654426449876927; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7295380241082725, learning_rate=0.001052037699531582, max_depth=7, n_estimators=332, scale_pos_weight=4.04781258158029, subsample=0.6329311706285883; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7295380241082725, learning_rate=0.001052037699531582, max_depth=7, n_estimators=332, scale_pos_weight=4.04781258158029, subsample=0.6329311706285883; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7295380241082725, learning_rate=0.001052037699531582, max_depth=7, n_estimators=332, scale_pos_weight=4.04781258158029, subsample=0.6329311706285883; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7068178838750884, learning_rate=0.04948299713589832, max_depth=3, n_estimators=406, scale_pos_weight=3.6941233379852148, subsample=0.6488251044495548; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7068178838750884, learning_rate=0.04948299713589832, max_depth=3, n_estimators=406, scale_pos_weight=3.6941233379852148, subsample=0.6488251044495548; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7068178838750884, learning_rate=0.04948299713589832, max_depth=3, n_estimators=406, scale_pos_weight=3.6941233379852148, subsample=0.6488251044495548; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6336582084345861, learning_rate=0.022876421957307026, max_depth=3, n_estimators=279, scale_pos_weight=4.63629602379294, subsample=0.7943564165441921; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6336582084345861, learning_rate=0.022876421957307026, max_depth=3, n_estimators=279, scale_pos_weight=4.63629602379294, subsample=0.7943564165441921; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.6336582084345861, learning_rate=0.022876421957307026, max_depth=3, n_estimators=279, scale_pos_weight=4.63629602379294, subsample=0.7943564165441921; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.7924894589884223, learning_rate=0.02617822958253642, max_depth=4, n_estimators=486, scale_pos_weight=4.008783098167696, subsample=0.6569680988754935; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7924894589884223, learning_rate=0.02617822958253642, max_depth=4, n_estimators=486, scale_pos_weight=4.008783098167696, subsample=0.6569680988754935; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7924894589884223, learning_rate=0.02617822958253642, max_depth=4, n_estimators=486, scale_pos_weight=4.008783098167696, subsample=0.6569680988754935; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.6073773894709066, learning_rate=0.06195643339798969, max_depth=4, n_estimators=319, scale_pos_weight=1.5147875124998935, subsample=0.6557292928473223; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6073773894709066, learning_rate=0.06195643339798969, max_depth=4, n_estimators=319, scale_pos_weight=1.5147875124998935, subsample=0.6557292928473223; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6073773894709066, learning_rate=0.06195643339798969, max_depth=4, n_estimators=319, scale_pos_weight=1.5147875124998935, subsample=0.6557292928473223; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7816531771933307, learning_rate=0.024956189066697245, max_depth=4, n_estimators=152, scale_pos_weight=5.89452760277563, subsample=0.7971300908221202; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7816531771933307, learning_rate=0.024956189066697245, max_depth=4, n_estimators=152, scale_pos_weight=5.89452760277563, subsample=0.7971300908221202; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7816531771933307, learning_rate=0.024956189066697245, max_depth=4, n_estimators=152, scale_pos_weight=5.89452760277563, subsample=0.7971300908221202; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6484110543023001, learning_rate=0.06821355474058786, max_depth=5, n_estimators=283, scale_pos_weight=3.3763754399239967, subsample=0.7456432697223719; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6484110543023001, learning_rate=0.06821355474058786, max_depth=5, n_estimators=283, scale_pos_weight=3.3763754399239967, subsample=0.7456432697223719; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6484110543023001, learning_rate=0.06821355474058786, max_depth=5, n_estimators=283, scale_pos_weight=3.3763754399239967, subsample=0.7456432697223719; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.6735566265438506, learning_rate=0.06423058305935796, max_depth=8, n_estimators=494, scale_pos_weight=9.16431873219384, subsample=0.7596690249969102; total time=   3.6s\n",
            "[CV] END colsample_bytree=0.6735566265438506, learning_rate=0.06423058305935796, max_depth=8, n_estimators=494, scale_pos_weight=9.16431873219384, subsample=0.7596690249969102; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.6735566265438506, learning_rate=0.06423058305935796, max_depth=8, n_estimators=494, scale_pos_weight=9.16431873219384, subsample=0.7596690249969102; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.6301435087930859, learning_rate=0.05181987767407187, max_depth=7, n_estimators=358, scale_pos_weight=6.908929431882418, subsample=0.7355128723684565; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6301435087930859, learning_rate=0.05181987767407187, max_depth=7, n_estimators=358, scale_pos_weight=6.908929431882418, subsample=0.7355128723684565; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6301435087930859, learning_rate=0.05181987767407187, max_depth=7, n_estimators=358, scale_pos_weight=6.908929431882418, subsample=0.7355128723684565; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6033175657855712, learning_rate=0.0522093058299281, max_depth=5, n_estimators=247, scale_pos_weight=7.451727904094499, subsample=0.6348732858009982; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6033175657855712, learning_rate=0.0522093058299281, max_depth=5, n_estimators=247, scale_pos_weight=7.451727904094499, subsample=0.6348732858009982; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6033175657855712, learning_rate=0.0522093058299281, max_depth=5, n_estimators=247, scale_pos_weight=7.451727904094499, subsample=0.6348732858009982; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.7381875476204932, learning_rate=0.03967353463005374, max_depth=4, n_estimators=459, scale_pos_weight=2.3752094414599325, subsample=0.6682132702100517; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7381875476204932, learning_rate=0.03967353463005374, max_depth=4, n_estimators=459, scale_pos_weight=2.3752094414599325, subsample=0.6682132702100517; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7381875476204932, learning_rate=0.03967353463005374, max_depth=4, n_estimators=459, scale_pos_weight=2.3752094414599325, subsample=0.6682132702100517; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.6226947042481178, learning_rate=0.09346936182785628, max_depth=8, n_estimators=397, scale_pos_weight=3.579416277151556, subsample=0.7319968092068359; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6226947042481178, learning_rate=0.09346936182785628, max_depth=8, n_estimators=397, scale_pos_weight=3.579416277151556, subsample=0.7319968092068359; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.6226947042481178, learning_rate=0.09346936182785628, max_depth=8, n_estimators=397, scale_pos_weight=3.579416277151556, subsample=0.7319968092068359; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7634444400402431, learning_rate=0.05652008115994624, max_depth=6, n_estimators=468, scale_pos_weight=3.418522909004517, subsample=0.6186205535611798; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7634444400402431, learning_rate=0.05652008115994624, max_depth=6, n_estimators=468, scale_pos_weight=3.418522909004517, subsample=0.6186205535611798; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.7634444400402431, learning_rate=0.05652008115994624, max_depth=6, n_estimators=468, scale_pos_weight=3.418522909004517, subsample=0.6186205535611798; total time=   0.5s\n",
            "[CV] END colsample_bytree=0.7794431515906654, learning_rate=0.09104180571633305, max_depth=5, n_estimators=108, scale_pos_weight=3.7887135259218185, subsample=0.7400715659945543; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.7794431515906654, learning_rate=0.09104180571633305, max_depth=5, n_estimators=108, scale_pos_weight=3.7887135259218185, subsample=0.7400715659945543; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.7794431515906654, learning_rate=0.09104180571633305, max_depth=5, n_estimators=108, scale_pos_weight=3.7887135259218185, subsample=0.7400715659945543; total time=   0.2s\n",
            "[CV] END colsample_bytree=0.7693322284476611, learning_rate=0.08663242918780925, max_depth=8, n_estimators=219, scale_pos_weight=9.877700987609598, subsample=0.7701856897535025; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7693322284476611, learning_rate=0.08663242918780925, max_depth=8, n_estimators=219, scale_pos_weight=9.877700987609598, subsample=0.7701856897535025; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7693322284476611, learning_rate=0.08663242918780925, max_depth=8, n_estimators=219, scale_pos_weight=9.877700987609598, subsample=0.7701856897535025; total time=   0.4s\n",
            "[CV] END colsample_bytree=0.7871269988441895, learning_rate=0.07953406511139437, max_depth=6, n_estimators=353, scale_pos_weight=1.0919705161662965, subsample=0.6202943085732064; total time=   1.7s\n",
            "[CV] END colsample_bytree=0.7871269988441895, learning_rate=0.07953406511139437, max_depth=6, n_estimators=353, scale_pos_weight=1.0919705161662965, subsample=0.6202943085732064; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.7871269988441895, learning_rate=0.07953406511139437, max_depth=6, n_estimators=353, scale_pos_weight=1.0919705161662965, subsample=0.6202943085732064; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.7327003538216111, learning_rate=0.0015061583846218687, max_depth=8, n_estimators=437, scale_pos_weight=5.856137535862266, subsample=0.6896848285972494; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7327003538216111, learning_rate=0.0015061583846218687, max_depth=8, n_estimators=437, scale_pos_weight=5.856137535862266, subsample=0.6896848285972494; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7327003538216111, learning_rate=0.0015061583846218687, max_depth=8, n_estimators=437, scale_pos_weight=5.856137535862266, subsample=0.6896848285972494; total time=   1.5s\n",
            "[CV] END colsample_bytree=0.7988914925221642, learning_rate=0.018592525267734538, max_depth=6, n_estimators=388, scale_pos_weight=5.938937151834346, subsample=0.6357645418442658; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.7988914925221642, learning_rate=0.018592525267734538, max_depth=6, n_estimators=388, scale_pos_weight=5.938937151834346, subsample=0.6357645418442658; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7988914925221642, learning_rate=0.018592525267734538, max_depth=6, n_estimators=388, scale_pos_weight=5.938937151834346, subsample=0.6357645418442658; total time=   0.9s\n",
            "[CV] END colsample_bytree=0.673293756916572, learning_rate=0.07541705230565623, max_depth=5, n_estimators=227, scale_pos_weight=6.683086033354716, subsample=0.6187349535656185; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.673293756916572, learning_rate=0.07541705230565623, max_depth=5, n_estimators=227, scale_pos_weight=6.683086033354716, subsample=0.6187349535656185; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.673293756916572, learning_rate=0.07541705230565623, max_depth=5, n_estimators=227, scale_pos_weight=6.683086033354716, subsample=0.6187349535656185; total time=   0.3s\n",
            "[CV] END colsample_bytree=0.6735431606118867, learning_rate=0.027520236768172546, max_depth=8, n_estimators=257, scale_pos_weight=10.730105547524456, subsample=0.6786195449333521; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.6735431606118867, learning_rate=0.027520236768172546, max_depth=8, n_estimators=257, scale_pos_weight=10.730105547524456, subsample=0.6786195449333521; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.6735431606118867, learning_rate=0.027520236768172546, max_depth=8, n_estimators=257, scale_pos_weight=10.730105547524456, subsample=0.6786195449333521; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7784093110354227, learning_rate=0.06411386259972629, max_depth=4, n_estimators=376, scale_pos_weight=6.026370931051921, subsample=0.7153807769252718; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.7784093110354227, learning_rate=0.06411386259972629, max_depth=4, n_estimators=376, scale_pos_weight=6.026370931051921, subsample=0.7153807769252718; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.7784093110354227, learning_rate=0.06411386259972629, max_depth=4, n_estimators=376, scale_pos_weight=6.026370931051921, subsample=0.7153807769252718; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.6985035387637728, learning_rate=0.020524298779804453, max_depth=7, n_estimators=403, scale_pos_weight=3.807723624408558, subsample=0.6048631932862908; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6985035387637728, learning_rate=0.020524298779804453, max_depth=7, n_estimators=403, scale_pos_weight=3.807723624408558, subsample=0.6048631932862908; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.6985035387637728, learning_rate=0.020524298779804453, max_depth=7, n_estimators=403, scale_pos_weight=3.807723624408558, subsample=0.6048631932862908; total time=   0.8s\n",
            "[CV] END colsample_bytree=0.7290944591814336, learning_rate=0.018711067940704897, max_depth=6, n_estimators=257, scale_pos_weight=10.539285770025874, subsample=0.7829728780440897; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7290944591814336, learning_rate=0.018711067940704897, max_depth=6, n_estimators=257, scale_pos_weight=10.539285770025874, subsample=0.7829728780440897; total time=   0.7s\n",
            "[CV] END colsample_bytree=0.7290944591814336, learning_rate=0.018711067940704897, max_depth=6, n_estimators=257, scale_pos_weight=10.539285770025874, subsample=0.7829728780440897; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.6740317400510889, learning_rate=0.002545661652886743, max_depth=7, n_estimators=401, scale_pos_weight=10.666548190436696, subsample=0.7927239954178505; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.6740317400510889, learning_rate=0.002545661652886743, max_depth=7, n_estimators=401, scale_pos_weight=10.666548190436696, subsample=0.7927239954178505; total time=   1.4s\n",
            "[CV] END colsample_bytree=0.6740317400510889, learning_rate=0.002545661652886743, max_depth=7, n_estimators=401, scale_pos_weight=10.666548190436696, subsample=0.7927239954178505; total time=   1.3s\n",
            "[CV] END colsample_bytree=0.770601891093472, learning_rate=0.03044488920695857, max_depth=7, n_estimators=260, scale_pos_weight=4.169220051562776, subsample=0.6338985493372185; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.770601891093472, learning_rate=0.03044488920695857, max_depth=7, n_estimators=260, scale_pos_weight=4.169220051562776, subsample=0.6338985493372185; total time=   0.6s\n",
            "[CV] END colsample_bytree=0.770601891093472, learning_rate=0.03044488920695857, max_depth=7, n_estimators=260, scale_pos_weight=4.169220051562776, subsample=0.6338985493372185; total time=   2.0s\n",
            "[CV] END colsample_bytree=0.71136025249167, learning_rate=0.0946154774160781, max_depth=4, n_estimators=422, scale_pos_weight=6.700611700893649, subsample=0.6194352987541537; total time=   2.1s\n",
            "[CV] END colsample_bytree=0.71136025249167, learning_rate=0.0946154774160781, max_depth=4, n_estimators=422, scale_pos_weight=6.700611700893649, subsample=0.6194352987541537; total time=   1.9s\n",
            "[CV] END colsample_bytree=0.71136025249167, learning_rate=0.0946154774160781, max_depth=4, n_estimators=422, scale_pos_weight=6.700611700893649, subsample=0.6194352987541537; total time=   0.4s\n",
            "Best parameters found:  {'colsample_bytree': 0.673293756916572, 'learning_rate': 0.07541705230565623, 'max_depth': 5, 'n_estimators': 227, 'scale_pos_weight': 6.683086033354716, 'subsample': 0.6187349535656185}\n",
            "Starting new iteration...\n",
            "Added 494 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 267 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 104 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 55 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 24 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 13 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 14 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 10 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 5 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 9 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 4 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 2 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 3 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 1 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 3 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "Added 1 pseudo-labeled samples.\n",
            "Starting new iteration...\n",
            "No confident predictions left. Stopping...\n",
            "Total pseudo-labeled data added: 1013\n",
            "Combined labeled data saved to combined_labeled_data_xgboost_tuned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Co-Training"
      ],
      "metadata": {
        "id": "zBty86Ch5Cpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "PwoJgoVg5LQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 400)\n",
        "        self.fc21 = nn.Linear(400, latent_dim)  # Mean of latent variable\n",
        "        self.fc22 = nn.Linear(400, latent_dim)  # Log-variance of latent variable\n",
        "        self.fc3 = nn.Linear(latent_dim, 400)\n",
        "        self.fc4 = nn.Linear(400, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)  # Returns mean and log-variance\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))  # Sigmoid to squash output between 0 and 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "    def loss_function(self, recon_x, x, mu, logvar):\n",
        "        BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "        # KL divergence between the learned distribution and a unit Gaussian\n",
        "        MSE = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        return BCE + MSE"
      ],
      "metadata": {
        "id": "_7jMalplIV6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifierWithVariationalDropout(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.5):\n",
        "        super(ClassifierWithVariationalDropout, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, output_dim)\n",
        "        self.dropout_rate = torch.nn.Parameter(torch.tensor([dropout_rate]))  # Learnable dropout rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.dropout_rate.item(), training=self.training)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "R4bRgZheIZFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### DATA PREPARATION ####\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Normalize gene and clinical data using MinMaxScaler\n",
        "scaler_gene = MinMaxScaler()\n",
        "X_gene_scaled = scaler_gene.fit_transform(x_train)  # Normalize labeled gene data\n",
        "X_gene_ul_scaled = scaler_gene.transform(x_ul)                # Unlabeled data\n",
        "\n",
        "scaler_clinical = MinMaxScaler()\n",
        "X_clinical_scaled = scaler_clinical.fit_transform(c_train)  # Normalize labeled clinical data\n",
        "\n",
        "# Convert labels (True/False) to integers (1/0)\n",
        "Y_train = (y_train == 'True').astype(int)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_gene_tensor = torch.tensor(X_gene_scaled, dtype=torch.float32)\n",
        "X_clinical_tensor = torch.tensor(X_clinical_scaled, dtype=torch.float32)\n",
        "Y_tensor = torch.tensor(Y_train, dtype=torch.long)  # Use long for classification targets\n",
        "\n",
        "# Split labeled data into training and testing sets\n",
        "X_gene_train, X_gene_test, Y_train, Y_test = train_test_split(X_gene_tensor, Y_tensor, test_size=0.2, random_state=42)\n",
        "X_clinical_train, X_clinical_test = train_test_split(X_clinical_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "train_data_gene = TensorDataset(X_gene_train, Y_train)\n",
        "test_data_gene = TensorDataset(X_gene_test, Y_test)\n",
        "\n",
        "train_data_clinical = TensorDataset(X_clinical_train, Y_train)\n",
        "test_data_clinical = TensorDataset(X_clinical_test, Y_test)\n",
        "\n",
        "train_loader_gene = DataLoader(train_data_gene, batch_size=32, shuffle=True)\n",
        "test_loader_gene = DataLoader(test_data_gene, batch_size=32, shuffle=False)\n",
        "\n",
        "train_loader_clinical = DataLoader(train_data_clinical, batch_size=32, shuffle=True)\n",
        "test_loader_clinical = DataLoader(test_data_clinical, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "tISk4IFxgpVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TRAINING ####\n",
        "\n",
        "# Initialize models\n",
        "vae = VAE(input_dim=X_gene_train.shape[1], latent_dim=10)  # VAE for gene data\n",
        "classifier_gene = ClassifierWithVariationalDropout(input_dim=10, output_dim=2)  # Classifier for VAE latent space\n",
        "classifier_clinical = ClassifierWithVariationalDropout(input_dim=X_clinical_train.shape[1], output_dim=2)  # Classifier for clinical data\n",
        "\n",
        "# Loss functions and optimizers\n",
        "vae_optimizer = torch.optim.Adam(list(vae.parameters()) + list(classifier_gene.parameters()), lr=0.1)\n",
        "classifier_optimizer = torch.optim.Adam(classifier_clinical.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "# Training Loop for VAE + Classifier on Gene Data\n",
        "for epoch in range(100):  # Adjust epochs as needed\n",
        "    vae.train()\n",
        "    classifier_gene.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_gene):\n",
        "        vae_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through VAE and classifier\n",
        "        recon_batch, mu, logvar = vae(data)\n",
        "        output = classifier_gene(mu)  # Use latent space representation\n",
        "\n",
        "        # Compute VAE loss and classification loss\n",
        "        vae_loss = vae.loss_function(recon_batch, data, mu, logvar)\n",
        "        classification_loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "        # Total loss\n",
        "        loss = vae_loss + classification_loss\n",
        "        loss.backward()\n",
        "        vae_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} | VAE Loss: {vae_loss.item()} | Classification Loss: {classification_loss.item()}\")\n",
        "\n",
        "print('##---------------------------------------------------------##')\n",
        "\n",
        "# Training Loop for Classifier on Clinical Data\n",
        "for epoch in range(100):  # Adjust epochs as needed\n",
        "    classifier_clinical.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader_clinical):\n",
        "        classifier_optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through classifier\n",
        "        output = classifier_clinical(data)\n",
        "\n",
        "        # Compute classification loss\n",
        "        classification_loss = torch.nn.functional.cross_entropy(output, target)\n",
        "\n",
        "        classification_loss.backward()\n",
        "        classifier_optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Clinical Classification Loss: {classification_loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M01Q6fOAIcWc",
        "outputId": "654f8726-7c74-499f-ec7c-7f8143217a53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | VAE Loss: 1121.533447265625 | Classification Loss: 0.0\n",
            "Epoch 1 | VAE Loss: 963.0816040039062 | Classification Loss: 0.0\n",
            "Epoch 2 | VAE Loss: 1478.345947265625 | Classification Loss: 0.0\n",
            "Epoch 3 | VAE Loss: 1292.28125 | Classification Loss: 0.0\n",
            "Epoch 4 | VAE Loss: 899.00341796875 | Classification Loss: 0.0\n",
            "Epoch 5 | VAE Loss: 819.0283813476562 | Classification Loss: 0.0\n",
            "Epoch 6 | VAE Loss: 771.4208374023438 | Classification Loss: 0.0\n",
            "Epoch 7 | VAE Loss: 523.0353393554688 | Classification Loss: 0.0\n",
            "Epoch 8 | VAE Loss: 350.8897705078125 | Classification Loss: 0.0\n",
            "Epoch 9 | VAE Loss: 298.42315673828125 | Classification Loss: 0.0\n",
            "Epoch 10 | VAE Loss: 315.5010681152344 | Classification Loss: 0.0\n",
            "Epoch 11 | VAE Loss: 749.5999755859375 | Classification Loss: 0.0\n",
            "Epoch 12 | VAE Loss: 687.9379272460938 | Classification Loss: 0.0\n",
            "Epoch 13 | VAE Loss: 602.1236572265625 | Classification Loss: 0.0\n",
            "Epoch 14 | VAE Loss: 472.87994384765625 | Classification Loss: 0.0\n",
            "Epoch 15 | VAE Loss: 374.14471435546875 | Classification Loss: 0.0\n",
            "Epoch 16 | VAE Loss: 937.9301147460938 | Classification Loss: 0.0\n",
            "Epoch 17 | VAE Loss: 401.3939514160156 | Classification Loss: 0.0\n",
            "Epoch 18 | VAE Loss: 375.0379638671875 | Classification Loss: 0.0\n",
            "Epoch 19 | VAE Loss: 397.80755615234375 | Classification Loss: 0.0\n",
            "Epoch 20 | VAE Loss: 397.4534912109375 | Classification Loss: 0.0\n",
            "Epoch 21 | VAE Loss: 328.1979675292969 | Classification Loss: 0.0\n",
            "Epoch 22 | VAE Loss: 293.8639221191406 | Classification Loss: 0.0\n",
            "Epoch 23 | VAE Loss: 314.25146484375 | Classification Loss: 0.0\n",
            "Epoch 24 | VAE Loss: 278.0353698730469 | Classification Loss: 0.0\n",
            "Epoch 25 | VAE Loss: 287.36968994140625 | Classification Loss: 0.0\n",
            "Epoch 26 | VAE Loss: 316.92767333984375 | Classification Loss: 0.0\n",
            "Epoch 27 | VAE Loss: 310.2543640136719 | Classification Loss: 0.0\n",
            "Epoch 28 | VAE Loss: 278.2579345703125 | Classification Loss: 0.0\n",
            "Epoch 29 | VAE Loss: 271.23773193359375 | Classification Loss: 0.0\n",
            "Epoch 30 | VAE Loss: 284.28839111328125 | Classification Loss: 0.0\n",
            "Epoch 31 | VAE Loss: 264.8668518066406 | Classification Loss: 0.0\n",
            "Epoch 32 | VAE Loss: 269.5000305175781 | Classification Loss: 0.0\n",
            "Epoch 33 | VAE Loss: 265.6475830078125 | Classification Loss: 0.0\n",
            "Epoch 34 | VAE Loss: 263.0876159667969 | Classification Loss: 0.0\n",
            "Epoch 35 | VAE Loss: 259.5146484375 | Classification Loss: 0.0\n",
            "Epoch 36 | VAE Loss: 258.50341796875 | Classification Loss: 0.0\n",
            "Epoch 37 | VAE Loss: 259.3909912109375 | Classification Loss: 0.0\n",
            "Epoch 38 | VAE Loss: 258.1694641113281 | Classification Loss: 0.0\n",
            "Epoch 39 | VAE Loss: 261.0873107910156 | Classification Loss: 0.0\n",
            "Epoch 40 | VAE Loss: 258.79498291015625 | Classification Loss: 0.0\n",
            "Epoch 41 | VAE Loss: 258.1961669921875 | Classification Loss: 0.0\n",
            "Epoch 42 | VAE Loss: 261.55438232421875 | Classification Loss: 0.0\n",
            "Epoch 43 | VAE Loss: 258.8401184082031 | Classification Loss: 0.0\n",
            "Epoch 44 | VAE Loss: 258.6800842285156 | Classification Loss: 0.0\n",
            "Epoch 45 | VAE Loss: 259.6057434082031 | Classification Loss: 0.0\n",
            "Epoch 46 | VAE Loss: 259.49072265625 | Classification Loss: 0.0\n",
            "Epoch 47 | VAE Loss: 261.42071533203125 | Classification Loss: 0.0\n",
            "Epoch 48 | VAE Loss: 258.27911376953125 | Classification Loss: 0.0\n",
            "Epoch 49 | VAE Loss: 259.72210693359375 | Classification Loss: 0.0\n",
            "Epoch 50 | VAE Loss: 260.9978332519531 | Classification Loss: 0.0\n",
            "Epoch 51 | VAE Loss: 259.0782775878906 | Classification Loss: 0.0\n",
            "Epoch 52 | VAE Loss: 258.9190979003906 | Classification Loss: 0.0\n",
            "Epoch 53 | VAE Loss: 260.6502990722656 | Classification Loss: 0.0\n",
            "Epoch 54 | VAE Loss: 256.8946838378906 | Classification Loss: 0.0\n",
            "Epoch 55 | VAE Loss: 257.3929748535156 | Classification Loss: 0.0\n",
            "Epoch 56 | VAE Loss: 258.5143737792969 | Classification Loss: 0.0\n",
            "Epoch 57 | VAE Loss: 257.4735412597656 | Classification Loss: 0.0\n",
            "Epoch 58 | VAE Loss: 257.4132080078125 | Classification Loss: 0.0\n",
            "Epoch 59 | VAE Loss: 260.1673583984375 | Classification Loss: 0.0\n",
            "Epoch 60 | VAE Loss: 257.8996276855469 | Classification Loss: 0.0\n",
            "Epoch 61 | VAE Loss: 259.0160217285156 | Classification Loss: 0.0\n",
            "Epoch 62 | VAE Loss: 257.319580078125 | Classification Loss: 0.0\n",
            "Epoch 63 | VAE Loss: 258.53955078125 | Classification Loss: 0.0\n",
            "Epoch 64 | VAE Loss: 260.91827392578125 | Classification Loss: 0.0\n",
            "Epoch 65 | VAE Loss: 257.9657897949219 | Classification Loss: 0.0\n",
            "Epoch 66 | VAE Loss: 260.8412780761719 | Classification Loss: 0.0\n",
            "Epoch 67 | VAE Loss: 255.37362670898438 | Classification Loss: 0.0\n",
            "Epoch 68 | VAE Loss: 263.3577575683594 | Classification Loss: 0.0\n",
            "Epoch 69 | VAE Loss: 258.55914306640625 | Classification Loss: 0.0\n",
            "Epoch 70 | VAE Loss: 257.2968444824219 | Classification Loss: 0.0\n",
            "Epoch 71 | VAE Loss: 258.95654296875 | Classification Loss: 0.0\n",
            "Epoch 72 | VAE Loss: 256.4480895996094 | Classification Loss: 0.0\n",
            "Epoch 73 | VAE Loss: 261.4436950683594 | Classification Loss: 0.0\n",
            "Epoch 74 | VAE Loss: 257.9643859863281 | Classification Loss: 0.0\n",
            "Epoch 75 | VAE Loss: 257.44366455078125 | Classification Loss: 0.0\n",
            "Epoch 76 | VAE Loss: 257.4886779785156 | Classification Loss: 0.0\n",
            "Epoch 77 | VAE Loss: 261.0364074707031 | Classification Loss: 0.0\n",
            "Epoch 78 | VAE Loss: 258.3114013671875 | Classification Loss: 0.0\n",
            "Epoch 79 | VAE Loss: 258.49188232421875 | Classification Loss: 0.0\n",
            "Epoch 80 | VAE Loss: 260.2555236816406 | Classification Loss: 0.0\n",
            "Epoch 81 | VAE Loss: 256.6293029785156 | Classification Loss: 0.0\n",
            "Epoch 82 | VAE Loss: 260.4599304199219 | Classification Loss: 0.0\n",
            "Epoch 83 | VAE Loss: 256.3936462402344 | Classification Loss: 0.0\n",
            "Epoch 84 | VAE Loss: 258.6105651855469 | Classification Loss: 0.0\n",
            "Epoch 85 | VAE Loss: 259.6563415527344 | Classification Loss: 0.0\n",
            "Epoch 86 | VAE Loss: 258.17047119140625 | Classification Loss: 0.0\n",
            "Epoch 87 | VAE Loss: 260.69384765625 | Classification Loss: 0.0\n",
            "Epoch 88 | VAE Loss: 260.4607238769531 | Classification Loss: 0.0\n",
            "Epoch 89 | VAE Loss: 261.0035400390625 | Classification Loss: 0.0\n",
            "Epoch 90 | VAE Loss: 257.6528015136719 | Classification Loss: 0.0\n",
            "Epoch 91 | VAE Loss: 259.18609619140625 | Classification Loss: 0.0\n",
            "Epoch 92 | VAE Loss: 253.83335876464844 | Classification Loss: 0.0\n",
            "Epoch 93 | VAE Loss: 258.8448181152344 | Classification Loss: 0.0\n",
            "Epoch 94 | VAE Loss: 259.8407287597656 | Classification Loss: 0.0\n",
            "Epoch 95 | VAE Loss: 258.992919921875 | Classification Loss: 0.0\n",
            "Epoch 96 | VAE Loss: 262.1825256347656 | Classification Loss: 0.0\n",
            "Epoch 97 | VAE Loss: 258.609130859375 | Classification Loss: 0.0\n",
            "Epoch 98 | VAE Loss: 256.9396057128906 | Classification Loss: 0.0\n",
            "Epoch 99 | VAE Loss: 257.1072082519531 | Classification Loss: 0.0\n",
            "##---------------------------------------------------------##\n",
            "Epoch 0 | Clinical Classification Loss: 0.8766263723373413\n",
            "Epoch 1 | Clinical Classification Loss: 0.8353144526481628\n",
            "Epoch 2 | Clinical Classification Loss: 0.785819411277771\n",
            "Epoch 3 | Clinical Classification Loss: 0.6818218231201172\n",
            "Epoch 4 | Clinical Classification Loss: 0.7220350503921509\n",
            "Epoch 5 | Clinical Classification Loss: 0.6374942064285278\n",
            "Epoch 6 | Clinical Classification Loss: 0.6190637350082397\n",
            "Epoch 7 | Clinical Classification Loss: 0.6245428323745728\n",
            "Epoch 8 | Clinical Classification Loss: 0.5347668528556824\n",
            "Epoch 9 | Clinical Classification Loss: 0.47971826791763306\n",
            "Epoch 10 | Clinical Classification Loss: 0.4798188805580139\n",
            "Epoch 11 | Clinical Classification Loss: 0.45243892073631287\n",
            "Epoch 12 | Clinical Classification Loss: 0.46492934226989746\n",
            "Epoch 13 | Clinical Classification Loss: 0.39304691553115845\n",
            "Epoch 14 | Clinical Classification Loss: 0.43648314476013184\n",
            "Epoch 15 | Clinical Classification Loss: 0.37396976351737976\n",
            "Epoch 16 | Clinical Classification Loss: 0.3268654942512512\n",
            "Epoch 17 | Clinical Classification Loss: 0.3175543248653412\n",
            "Epoch 18 | Clinical Classification Loss: 0.3658387064933777\n",
            "Epoch 19 | Clinical Classification Loss: 0.28382372856140137\n",
            "Epoch 20 | Clinical Classification Loss: 0.2512471675872803\n",
            "Epoch 21 | Clinical Classification Loss: 0.26217585802078247\n",
            "Epoch 22 | Clinical Classification Loss: 0.23923084139823914\n",
            "Epoch 23 | Clinical Classification Loss: 0.2844129502773285\n",
            "Epoch 24 | Clinical Classification Loss: 0.21873632073402405\n",
            "Epoch 25 | Clinical Classification Loss: 0.20736877620220184\n",
            "Epoch 26 | Clinical Classification Loss: 0.20659542083740234\n",
            "Epoch 27 | Clinical Classification Loss: 0.17611007392406464\n",
            "Epoch 28 | Clinical Classification Loss: 0.14994975924491882\n",
            "Epoch 29 | Clinical Classification Loss: 0.1782732605934143\n",
            "Epoch 30 | Clinical Classification Loss: 0.14712785184383392\n",
            "Epoch 31 | Clinical Classification Loss: 0.1606287807226181\n",
            "Epoch 32 | Clinical Classification Loss: 0.16131646931171417\n",
            "Epoch 33 | Clinical Classification Loss: 0.13334618508815765\n",
            "Epoch 34 | Clinical Classification Loss: 0.12960085272789001\n",
            "Epoch 35 | Clinical Classification Loss: 0.09203805029392242\n",
            "Epoch 36 | Clinical Classification Loss: 0.09288166463375092\n",
            "Epoch 37 | Clinical Classification Loss: 0.09662120044231415\n",
            "Epoch 38 | Clinical Classification Loss: 0.11883624643087387\n",
            "Epoch 39 | Clinical Classification Loss: 0.11174635589122772\n",
            "Epoch 40 | Clinical Classification Loss: 0.08897672593593597\n",
            "Epoch 41 | Clinical Classification Loss: 0.09858907014131546\n",
            "Epoch 42 | Clinical Classification Loss: 0.09436193853616714\n",
            "Epoch 43 | Clinical Classification Loss: 0.08570738136768341\n",
            "Epoch 44 | Clinical Classification Loss: 0.08514674752950668\n",
            "Epoch 45 | Clinical Classification Loss: 0.0823785811662674\n",
            "Epoch 46 | Clinical Classification Loss: 0.06139618903398514\n",
            "Epoch 47 | Clinical Classification Loss: 0.05901036411523819\n",
            "Epoch 48 | Clinical Classification Loss: 0.05535024404525757\n",
            "Epoch 49 | Clinical Classification Loss: 0.061671219766139984\n",
            "Epoch 50 | Clinical Classification Loss: 0.06323401629924774\n",
            "Epoch 51 | Clinical Classification Loss: 0.06459308415651321\n",
            "Epoch 52 | Clinical Classification Loss: 0.05273604393005371\n",
            "Epoch 53 | Clinical Classification Loss: 0.044129081070423126\n",
            "Epoch 54 | Clinical Classification Loss: 0.049419987946748734\n",
            "Epoch 55 | Clinical Classification Loss: 0.04324675351381302\n",
            "Epoch 56 | Clinical Classification Loss: 0.04975631460547447\n",
            "Epoch 57 | Clinical Classification Loss: 0.039065372198820114\n",
            "Epoch 58 | Clinical Classification Loss: 0.045907340943813324\n",
            "Epoch 59 | Clinical Classification Loss: 0.03028509020805359\n",
            "Epoch 60 | Clinical Classification Loss: 0.034709058701992035\n",
            "Epoch 61 | Clinical Classification Loss: 0.03196734935045242\n",
            "Epoch 62 | Clinical Classification Loss: 0.025366151705384254\n",
            "Epoch 63 | Clinical Classification Loss: 0.03370729088783264\n",
            "Epoch 64 | Clinical Classification Loss: 0.021185152232646942\n",
            "Epoch 65 | Clinical Classification Loss: 0.03125537559390068\n",
            "Epoch 66 | Clinical Classification Loss: 0.029263416305184364\n",
            "Epoch 67 | Clinical Classification Loss: 0.031105905771255493\n",
            "Epoch 68 | Clinical Classification Loss: 0.03608360141515732\n",
            "Epoch 69 | Clinical Classification Loss: 0.029780616983771324\n",
            "Epoch 70 | Clinical Classification Loss: 0.027452293783426285\n",
            "Epoch 71 | Clinical Classification Loss: 0.02838008664548397\n",
            "Epoch 72 | Clinical Classification Loss: 0.022871199995279312\n",
            "Epoch 73 | Clinical Classification Loss: 0.023684140294790268\n",
            "Epoch 74 | Clinical Classification Loss: 0.014099955558776855\n",
            "Epoch 75 | Clinical Classification Loss: 0.015487154945731163\n",
            "Epoch 76 | Clinical Classification Loss: 0.028104418888688087\n",
            "Epoch 77 | Clinical Classification Loss: 0.01833023875951767\n",
            "Epoch 78 | Clinical Classification Loss: 0.019308697432279587\n",
            "Epoch 79 | Clinical Classification Loss: 0.014285845682024956\n",
            "Epoch 80 | Clinical Classification Loss: 0.01915092021226883\n",
            "Epoch 81 | Clinical Classification Loss: 0.012306932359933853\n",
            "Epoch 82 | Clinical Classification Loss: 0.013245835900306702\n",
            "Epoch 83 | Clinical Classification Loss: 0.012780104763805866\n",
            "Epoch 84 | Clinical Classification Loss: 0.01496527623385191\n",
            "Epoch 85 | Clinical Classification Loss: 0.013266315683722496\n",
            "Epoch 86 | Clinical Classification Loss: 0.017511216923594475\n",
            "Epoch 87 | Clinical Classification Loss: 0.020065467804670334\n",
            "Epoch 88 | Clinical Classification Loss: 0.011296140030026436\n",
            "Epoch 89 | Clinical Classification Loss: 0.011090929619967937\n",
            "Epoch 90 | Clinical Classification Loss: 0.015617976896464825\n",
            "Epoch 91 | Clinical Classification Loss: 0.010630739852786064\n",
            "Epoch 92 | Clinical Classification Loss: 0.011267177760601044\n",
            "Epoch 93 | Clinical Classification Loss: 0.00907077081501484\n",
            "Epoch 94 | Clinical Classification Loss: 0.011663850396871567\n",
            "Epoch 95 | Clinical Classification Loss: 0.005524204578250647\n",
            "Epoch 96 | Clinical Classification Loss: 0.0068893348798155785\n",
            "Epoch 97 | Clinical Classification Loss: 0.011811691336333752\n",
            "Epoch 98 | Clinical Classification Loss: 0.008031257428228855\n",
            "Epoch 99 | Clinical Classification Loss: 0.010029228404164314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for pseudo-labeling\n",
        "def pseudo_labeling(model_gene, model_clinical, unlabeled_data_gene, unlabeled_data_clinical, threshold=0.8):\n",
        "    model_gene.eval()\n",
        "    model_clinical.eval()\n",
        "\n",
        "    # Pseudo-labeling for gene model\n",
        "    with torch.no_grad():\n",
        "        gene_preds = model_gene(unlabeled_data_gene)\n",
        "        confidence_gene, predicted_gene = torch.max(gene_preds, 1)\n",
        "        confident_gene_indices = confidence_gene > threshold\n",
        "\n",
        "    # Pseudo-labeling for clinical model\n",
        "    with torch.no_grad():\n",
        "        clinical_preds = model_clinical(unlabeled_data_clinical)\n",
        "        confidence_clinical, predicted_clinical = torch.max(clinical_preds, 1)\n",
        "        confident_clinical_indices = confidence_clinical > threshold\n",
        "\n",
        "    # Augment the labeled data with the confident pseudo-labels\n",
        "    pseudo_labeled_gene = unlabeled_data_gene[confident_gene_indices]\n",
        "    pseudo_labeled_clinical = unlabeled_data_clinical[confident_clinical_indices]\n",
        "\n",
        "    return pseudo_labeled_gene, pseudo_labeled_clinical\n"
      ],
      "metadata": {
        "id": "WTG7jjdXIeZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}